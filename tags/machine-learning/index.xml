<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>machine-learning on Stephen Malina</title>
    <link>https://an1lam.github.io/tags/machine-learning/</link>
    <description>Recent content in machine-learning on Stephen Malina</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 01 Nov 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://an1lam.github.io/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Let&#39;s Derive - Expectation Maximization</title>
      <link>https://an1lam.github.io/post/2019-11-01-baum-welch/</link>
      <pubDate>Fri, 01 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://an1lam.github.io/post/2019-11-01-baum-welch/</guid>
      <description>I&amp;rsquo;ve gotten so tired of reading other people&amp;rsquo;s descriptions of Expectation Maximization that don&amp;rsquo;t seem to help me understand it better so I&amp;rsquo;ve decided to write my own. Hopefully, this makes it so that I never have to read anyone else&amp;rsquo;s description again.
Reader Assumptions Since I&amp;rsquo;m writing this for myself, I&amp;rsquo;m going to assume a lot of knowledge. This is intended to be a prelude to a follow-up post in which I&amp;rsquo;d like to derive the Baum-Welch algorithm for learning Hidden Markov Model (HMM) parameters from data.</description>
    </item>
    
    <item>
      <title>Matrix Potpourri</title>
      <link>https://an1lam.github.io/post/2019-09-07-matrix-potpourri/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://an1lam.github.io/post/2019-09-07-matrix-potpourri/</guid>
      <description>Matrix Potpourri As part of reviewing Linear Algebra for my Machine Learning class, I&amp;rsquo;ve noticed there&amp;rsquo;s a bunch of matrix terminology that I didn&amp;rsquo;t encounter during my proof-based self-study of LA from Linear Algebra Done Right. This post is mostly intended to consolidate my own understanding and to act as a reference to future me, but if it also helps others in a similar position, that&amp;rsquo;s even better!
Note: I list all the sources from which I drew while writing this post under the &amp;ldquo;Sources&amp;rdquo; heading at the bottom of this post.</description>
    </item>
    
    <item>
      <title>Paper Review - DeepSEA</title>
      <link>https://an1lam.github.io/post/2019-08-08-deepsea/</link>
      <pubDate>Thu, 08 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://an1lam.github.io/post/2019-08-08-deepsea/</guid>
      <description>In which I record my thoughts on DeepSEA.
Terminology  ChIP-seq (Chromatin Immunoprecipitation Sequencing): Expression Quantitative Trait Locis (eQTLs) Cofactor Binding Sequences Histone Marks: modifications to histone proteins in the nucleosome that impact the shape of chromatin. gkm-SVMs: The gkm-SVM is the previous SOTA model for predicting transcription factor binding based on ChIP-seq data. Allele: variants of a gene at the same position on a chromosome.  What is this paper about?</description>
    </item>
    
    <item>
      <title>Paper Review - Basset</title>
      <link>https://an1lam.github.io/post/2019-08-05-basset/</link>
      <pubDate>Mon, 05 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://an1lam.github.io/post/2019-08-05-basset/</guid>
      <description>In which I record my thoughts on Basset.
Bio Background The genome consists of (broadly) two types of genes, coding genes and noncoding genes. Coding genes get translated into proteins (as laid down by the Central Dogma) and are what most of us learned about in bio class. Non-coding genes&amp;hellip; don&amp;rsquo;t. As I understand it, noncoding genes can do a bunch of different things, but part of their function is to regulate coding gene activity through a number of mechanisms.</description>
    </item>
    
    <item>
      <title>Paper Review - DeepBind</title>
      <link>https://an1lam.github.io/post/2019-07-26-deepbind/</link>
      <pubDate>Fri, 26 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://an1lam.github.io/post/2019-07-26-deepbind/</guid>
      <description>In which I record my thoughts on DeepBind.
What is this paper about? The authors of this paper designed a conv net model to predict how well different proteins will bind to sequences of DNA or RNA. They train one model per protein for many different sequences and show that these models can predict binding affinities for sequences well enough to produce insights regarding the impact of single nucleotide mutations.</description>
    </item>
    
  </channel>
</rss>