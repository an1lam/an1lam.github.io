<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Notes on Stephen Malina</title>
    <link>https://stephenmalina.com/tags/notes/</link>
    <description>Recent content in Notes on Stephen Malina</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 12 Dec 2019 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://stephenmalina.com/tags/notes/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>All of Statistics - Chapter 3</title>
      <link>https://stephenmalina.com/learning/2019-12-12-all-of-statistics-ch3-notes/</link>
      <pubDate>Thu, 12 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://stephenmalina.com/learning/2019-12-12-all-of-statistics-ch3-notes/</guid>
      <description>&lt;h1 id=&#34;selected-exercises&#34;&gt;&#xA;Selected Exercises&#xA;&lt;a href=&#34;#selected-exercises&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;strong&gt;1.&lt;/strong&gt; Suppose we play a game where we start with $ c $ dollars. On each play of the game you either double or halve your money, with equal probability. What is your expected fortune after $ n $ trials?&lt;/p&gt;&#xA;&lt;p&gt;I&amp;rsquo;m pretty sure we can use a trick that I&amp;rsquo;ve seen a lot in machine learning to solve this using plain old expectations. If I&amp;rsquo;m right, we let $ X \sim \text{Binomial}(n, \frac{1}{2}) $ and can create a new random variable, $ Z $, where&lt;/p&gt;</description>
    </item>
    <item>
      <title>Paper Review - Network Mendelian Randomization</title>
      <link>https://stephenmalina.com/post/2019-11-16-network-mendelian-randomization/</link>
      <pubDate>Sat, 16 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://stephenmalina.com/post/2019-11-16-network-mendelian-randomization/</guid>
      <description>&lt;p&gt;In which I record my thoughts on &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4469795/&#34;&gt;Network Mendelian Randomization&lt;/a&gt; by Burgess et al.&lt;/p&gt;&#xA;&lt;h2 id=&#34;what-is-this-paper-about&#34;&gt;&#xA;What is this paper about?&#xA;&lt;a href=&#34;#what-is-this-paper-about&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;This paper describes a method for doing Mendelian Randomization (MR) in the presence of a potential mediating variable. In the typical MR setting, we have an instrumental variable which &amp;ldquo;instruments&amp;rdquo; an exposure that we believe causally influences the outcome we care about. True mediators &amp;ldquo;mediate&amp;rdquo; the causal influence of exposures on outcomes. In other words, if we&amp;rsquo;re trying to understand the causal influence of some factor on another factor, it&amp;rsquo;s possible there&amp;rsquo;s a factor (mediator) that the exposure influences that then influences the outcome.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Causal Inference Notes</title>
      <link>https://stephenmalina.com/learning/2019-11-08-causal-inference/</link>
      <pubDate>Fri, 15 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://stephenmalina.com/learning/2019-11-08-causal-inference/</guid>
      <description>&lt;h2 id=&#34;causal-inference-in-statistics&#34;&gt;&#xA;Causal Inference in Statistics&#xA;&lt;a href=&#34;#causal-inference-in-statistics&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h3 id=&#34;questions&#34;&gt;&#xA;Questions&#xA;&lt;a href=&#34;#questions&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Why is the causal effect identifiable in an IV DAG when the dependencies are linear (from an SCM perspective)?&#xA;&lt;ul&gt;&#xA;&lt;li&gt;I think this may relate to factor models.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;advanced-data-analysis-from-an-elementary-point-of-view-ch-18-23&#34;&gt;&#xA;Advanced Data Analysis from an Elementary Point of View (Ch. 18-23)&#xA;&lt;a href=&#34;#advanced-data-analysis-from-an-elementary-point-of-view-ch-18-23&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h3 id=&#34;chapter-18&#34;&gt;&#xA;Chapter 18&#xA;&lt;a href=&#34;#chapter-18&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;h4 id=&#34;exercises&#34;&gt;&#xA;Exercises&#xA;&lt;a href=&#34;#exercises&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h4&gt;&#xA;&lt;p&gt;&lt;strong&gt;18.2.&lt;/strong&gt;&#xA;&lt;em&gt;Proof that every path must go through a collider:&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;Observe that every path between two exogenous variables starts with variables going in the opposite direction. Thus, if we imagine walking the path from one exogenous variable to another, we see that the direction of the arrows has to switch at a node, which will be a collider.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Paper Review - IVs and Mendelian Randomization</title>
      <link>https://stephenmalina.com/learning/2019-11-02-ivs-mendelian-randomization/</link>
      <pubDate>Sun, 03 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://stephenmalina.com/learning/2019-11-02-ivs-mendelian-randomization/</guid>
      <description>&lt;h1 id=&#34;summary&#34;&gt;&#xA;Summary&#xA;&lt;a href=&#34;#summary&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h1 id=&#34;detailed-notes&#34;&gt;&#xA;Detailed Notes&#xA;&lt;a href=&#34;#detailed-notes&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h2 id=&#34;3-iv-requirements&#34;&gt;&#xA;3 IV Requirements&#xA;&lt;a href=&#34;#3-iv-requirements&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;IV must have a direct influence on the treatment&lt;/li&gt;&#xA;&lt;li&gt;IV must not covary with the unmeasured confounding that impacts the outcome&lt;/li&gt;&#xA;&lt;li&gt;IV must not have a direct influence on the outcome&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;relevant-considerations-for-deciding-whether-to-do-an-iv-analysis&#34;&gt;&#xA;Relevant considerations for deciding whether to do an IV analysis&#xA;&lt;a href=&#34;#relevant-considerations-for-deciding-whether-to-do-an-iv-analysis&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Is there any unmeasured confounding?&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;If not, is the answer that we can just use a normal regression?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;When is unmeasured confounding especially likely?&lt;/p&gt;</description>
    </item>
    <item>
      <title>Paper Review - DeepSEA</title>
      <link>https://stephenmalina.com/post/2019-08-08-deepsea/</link>
      <pubDate>Thu, 08 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://stephenmalina.com/post/2019-08-08-deepsea/</guid>
      <description>&lt;p&gt;In which I record my thoughts on &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4768299/pdf/nihms757739.pdf&#34;&gt;DeepSEA&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;h2 id=&#34;terminology&#34;&gt;&#xA;Terminology&#xA;&lt;a href=&#34;#terminology&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;ChIP-seq (Chromatin Immunoprecipitation Sequencing):&lt;/li&gt;&#xA;&lt;li&gt;Expression Quantitative Trait Locis (eQTLs)&lt;/li&gt;&#xA;&lt;li&gt;Cofactor Binding Sequences&lt;/li&gt;&#xA;&lt;li&gt;Histone Marks: modifications to histone proteins in the nucleosome that impact the shape of chromatin.&lt;/li&gt;&#xA;&lt;li&gt;gkm-SVMs: The gkm-SVM is the previous SOTA model for predicting transcription factor binding based on ChIP-seq data.&lt;/li&gt;&#xA;&lt;li&gt;Allele: variants of a gene at the same position on a chromosome.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;what-is-this-paper-about&#34;&gt;&#xA;What is this paper about?&#xA;&lt;a href=&#34;#what-is-this-paper-about&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;This paper trains a CNN to predict the presence of 919 &amp;ldquo;chromatin features&amp;rdquo;&amp;ndash;different TF binding sites, DHSs, and histone marks&amp;ndash;from 1000bp DNA sequences. It then tests this model by using a functional significance score based on its output to train a logistic regression classifier to predict whether SNPs will be present in a few different catalog of SNPs known to impact different biological functions, e.g. a GWAS catalog of disease-related SNPs.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Paper Review - Basset</title>
      <link>https://stephenmalina.com/post/2019-08-05-basset/</link>
      <pubDate>Mon, 05 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://stephenmalina.com/post/2019-08-05-basset/</guid>
      <description>&lt;p&gt;In which I record my thoughts on &lt;a href=&#34;https://genome.cshlp.org/content/26/7/990.full&#34;&gt;Basset&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;h2 id=&#34;bio-background&#34;&gt;&#xA;Bio Background&#xA;&lt;a href=&#34;#bio-background&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;The genome consists of (broadly) two types of genes, coding genes and noncoding genes. Coding genes get translated into proteins (as laid down by the Central Dogma) and are what most of us learned about in bio class. Non-coding genes&amp;hellip; don&amp;rsquo;t. As I understand it, noncoding genes can do a bunch of different things, but part of their function is to regulate coding gene activity through a number of mechanisms.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Paper Review - DeepBind</title>
      <link>https://stephenmalina.com/post/2019-07-26-deepbind/</link>
      <pubDate>Fri, 26 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://stephenmalina.com/post/2019-07-26-deepbind/</guid>
      <description>&lt;p&gt;In which I record my thoughts on &lt;a href=&#34;https://www.nature.com/articles/nbt.3300.pdf&#34;&gt;DeepBind&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;h2 id=&#34;what-is-this-paper-about&#34;&gt;&#xA;What is this paper about?&#xA;&lt;a href=&#34;#what-is-this-paper-about&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;The authors of this paper designed a conv net model to predict how well different proteins will bind to sequences of DNA or RNA. They train one model per protein for many different sequences and show that these models can predict binding affinities for sequences well enough to produce insights regarding the impact of single nucleotide mutations. They then discuss different datasets and areas in which they tested their model and how it performed (often SOTA [as of 2015]).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Linear Algebra Done Right - Chapter 6</title>
      <link>https://stephenmalina.com/learning/2019-05-20-linear-algebra-done-right-ch6-notes/</link>
      <pubDate>Mon, 20 May 2019 00:00:00 +0000</pubDate>
      <guid>https://stephenmalina.com/learning/2019-05-20-linear-algebra-done-right-ch6-notes/</guid>
      <description>&lt;h1 id=&#34;selected-exercises&#34;&gt;&#xA;Selected Exercises&#xA;&lt;a href=&#34;#selected-exercises&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h2 id=&#34;6b&#34;&gt;&#xA;6.B&#xA;&lt;a href=&#34;#6b&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;1.&lt;/strong&gt; (a) Suppose \( \theta \in \mathbf{R} \). Show that $ (\cos \theta, \sin \theta), (-\sin \theta, \cos \theta) $ and $ (\cos \theta, \sin \theta), (\sin \theta, -\cos \theta) $ are orthonormal bases of $ \mathbf{R}^2 $.&lt;br&gt;&#xA;(b) Show that each orthonormal basis of $ \mathbf{R}^2 $ is of the form given by one of the two possibilities of part (a).&lt;/p&gt;&#xA;&lt;p&gt;(a) First, we show that both lists of vectors are orthonormal (using the Euclidean inner product), i.e. that the inner product of the two vectors equals 0 and that the inner product of each with itself equals 1.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Linear Algebra Done Right - Chapter 5</title>
      <link>https://stephenmalina.com/learning/2019-04-01-linear-algebra-done-right-ch5-notes/</link>
      <pubDate>Mon, 01 Apr 2019 00:00:00 +0000</pubDate>
      <guid>https://stephenmalina.com/learning/2019-04-01-linear-algebra-done-right-ch5-notes/</guid>
      <description>&lt;h1 id=&#34;selected-exercises&#34;&gt;&#xA;Selected Exercises&#xA;&lt;a href=&#34;#selected-exercises&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h2 id=&#34;5a&#34;&gt;&#xA;5.A&#xA;&lt;a href=&#34;#5a&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;12.&lt;/strong&gt; Define $ T \in \mathcal L(\mathcal P_4(\mathbf{R})) $ by&lt;/p&gt;&#xA;&lt;p&gt;$$&#xA;(Tp)(x) = xp&amp;rsquo;(x)&#xA;$$&lt;/p&gt;&#xA;&lt;p&gt;for all $ x \in \mathbf{R} $. Find all eigenvalues and eigenvectors of $ T $.&lt;/p&gt;&#xA;&lt;p&gt;Observe that, if $ p = a_0 + a_1 x + a_2 x^2 + a_3 x^3 + a_4 x^4 $, then&#xA;$$&#xA;x p&amp;rsquo;(x) = a_1 x + 2 a_2 x^2 + 3 a_3 x^3 + 4 a_4 x^4.&#xA;$$&lt;/p&gt;</description>
    </item>
    <item>
      <title>Linear Algebra Done Right - Chapter 4</title>
      <link>https://stephenmalina.com/learning/2019-03-31-linear-algebra-done-right-ch4-notes/</link>
      <pubDate>Sun, 31 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://stephenmalina.com/learning/2019-03-31-linear-algebra-done-right-ch4-notes/</guid>
      <description>&lt;h1 id=&#34;selected-exercises&#34;&gt;&#xA;Selected Exercises&#xA;&lt;a href=&#34;#selected-exercises&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;strong&gt;4.&lt;/strong&gt; Suppose $m$ and $n$ are positive integers with $ m \leq n $, and suppose $ \lambda_1, \dots, \lambda_m \in F $. Prove that there exists a polynomial $ p \in \mathcal P(\mathbf{F}) $ with $ \deg p = n $ such that $ 0 = p(\lambda_1) = \cdots = p(\lambda_m) $ and such that $ p $ has no other zeros.&lt;/p&gt;&#xA;&lt;p&gt;First, we can show that the polynomial $p&amp;rsquo;(z) = (z - \lambda_1) \cdots (z-\lambda_m) $ with $ \deg p&amp;rsquo; = m $ has $ 0 = p&amp;rsquo;(\lambda_1) = \cdots = p&amp;rsquo;(\lambda_m) $ and no other zeros. By 4.12, we know that $ p&amp;rsquo; $ has at most $ m $ zeros and therefore has no other zeros besides $ \lambda_1, \dots, \lambda_m $.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Linear Algebra Done Right - Chapter 3</title>
      <link>https://stephenmalina.com/learning/2019-03-30-linear-algebra-done-right-ch3-notes/</link>
      <pubDate>Sat, 30 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://stephenmalina.com/learning/2019-03-30-linear-algebra-done-right-ch3-notes/</guid>
      <description>&lt;h1 id=&#34;selected-exercises&#34;&gt;&#xA;Selected Exercises&#xA;&lt;a href=&#34;#selected-exercises&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h2 id=&#34;3d&#34;&gt;&#xA;3.D&#xA;&lt;a href=&#34;#3d&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;7.&lt;/strong&gt; Suppose $ V $ &amp;amp; $ W $ are finite-dimensional. Let $ v \in V $. Let&#xA;$$&#xA;E = \{ T \in \mathcal L(V, W): T(v) = 0 \}.&#xA;$$&lt;/p&gt;&#xA;&lt;p&gt;(a)  Show that $ E $ is a subspace of $ \mathcal L(V, W) $.&lt;br&gt;&#xA;(b)  Suppose $ v \neq 0 $. What is $\dim E$?&lt;/p&gt;&#xA;&lt;p&gt;For (a), to show $ E $ is a subspace of $ \mathcal L(V, W) $, we need to show that $ E $ contains zero and is closed under both addition and multiplication.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
