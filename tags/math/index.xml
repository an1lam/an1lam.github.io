<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Math on Stephen Malina</title>
    <link>http://localhost:1314/tags/math/</link>
    <description>Recent content in Math on Stephen Malina</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 09 Mar 2020 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1314/tags/math/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Deriving the front-door criterion with the do-calculus</title>
      <link>http://localhost:1314/post/2020-03-09-front-door-do-calc-derivation/</link>
      <pubDate>Mon, 09 Mar 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1314/post/2020-03-09-front-door-do-calc-derivation/</guid>
      <description>&lt;p&gt;&lt;em&gt;Attention conservation notice:&lt;/em&gt; Narrow target audience - will only make sense to people somewhat familiar with causal inference who don&amp;rsquo;t find the result entirely boring.&lt;/p&gt;&#xA;&lt;h2 id=&#34;the-front-door-criterion&#34;&gt;&#xA;The Front-Door Criterion&#xA;&lt;a href=&#34;#the-front-door-criterion&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;Suppose we have a causal graphical model that looks like the following.&lt;/p&gt;&#xA;&lt;img src=&#34;https://i.imgur.com/i5HQ1Jt.png&#34; width=&#34;500&#34; height=&#34;350&#34; /&gt;&#xA;&lt;p&gt;Assume $ U $ is unmeasured whereas $ X, M, Y $ can be measured. Notice that:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;All directed paths from $ X $ to $ Y $ flow through $ M $.&lt;/li&gt;&#xA;&lt;li&gt;$ X $ blocks all back-door paths from $ M $ to $ Y $.&lt;/li&gt;&#xA;&lt;li&gt;There are no unblocked back-door paths from $ X $ to $ M $.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;One of the most striking results in early causal inference literature, called the front-door criterion, states that, for all graphs like ours which satisfy these three criteria, the causal effect $ P(y \mid \mathrm{do}(x)) $ is identifiable by the formula (assume discrete variables for convenience)&lt;/p&gt;</description>
    </item>
    <item>
      <title>Matrix Potpourri</title>
      <link>http://localhost:1314/post/2019-09-07-matrix-potpourri/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1314/post/2019-09-07-matrix-potpourri/</guid>
      <description>&lt;h1 id=&#34;matrix-potpourri&#34;&gt;&#xA;Matrix Potpourri&#xA;&lt;a href=&#34;#matrix-potpourri&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;As part of reviewing Linear Algebra for my Machine Learning class, I&amp;rsquo;ve noticed there&amp;rsquo;s a bunch of matrix terminology that I didn&amp;rsquo;t encounter during my proof-based self-study of LA from &lt;a href=&#34;https://www.amazon.com/Programmers-Introduction-Mathematics-Dr-Jeremy/dp/1727125452/ref=asc_df_1727125452/?tag=hyprod-20&amp;amp;linkCode=df0&amp;amp;hvadid=312168166316&amp;amp;hvpos=1o1&amp;amp;hvnetw=g&amp;amp;hvrand=8153319341318076586&amp;amp;hvpone=&amp;amp;hvptwo=&amp;amp;hvqmt=&amp;amp;hvdev=c&amp;amp;hvdvcmdl=&amp;amp;hvlocint=&amp;amp;hvlocphy=9067609&amp;amp;hvtargid=aud-801381245258:pla-582015251962&amp;amp;psc=1&#34;&gt;&lt;em&gt;Linear Algebra Done Right&lt;/em&gt;&lt;/a&gt;. This post is mostly intended to consolidate my own understanding and to act as a reference to future me, but if it also helps others in a similar position, that&amp;rsquo;s even better!&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: I list all the sources from which I drew while writing this post under the &amp;ldquo;Sources&amp;rdquo; heading at the bottom of this post.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Linear Algebra Done Right - Chapter 6</title>
      <link>http://localhost:1314/learning/2019-05-20-linear-algebra-done-right-ch6-notes/</link>
      <pubDate>Mon, 20 May 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1314/learning/2019-05-20-linear-algebra-done-right-ch6-notes/</guid>
      <description>&lt;h1 id=&#34;selected-exercises&#34;&gt;&#xA;Selected Exercises&#xA;&lt;a href=&#34;#selected-exercises&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h2 id=&#34;6b&#34;&gt;&#xA;6.B&#xA;&lt;a href=&#34;#6b&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;1.&lt;/strong&gt; (a) Suppose \( \theta \in \mathbf{R} \). Show that $ (\cos \theta, \sin \theta), (-\sin \theta, \cos \theta) $ and $ (\cos \theta, \sin \theta), (\sin \theta, -\cos \theta) $ are orthonormal bases of $ \mathbf{R}^2 $.&lt;br&gt;&#xA;(b) Show that each orthonormal basis of $ \mathbf{R}^2 $ is of the form given by one of the two possibilities of part (a).&lt;/p&gt;&#xA;&lt;p&gt;(a) First, we show that both lists of vectors are orthonormal (using the Euclidean inner product), i.e. that the inner product of the two vectors equals 0 and that the inner product of each with itself equals 1.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Linear Algebra Done Right - Chapter 5</title>
      <link>http://localhost:1314/learning/2019-04-01-linear-algebra-done-right-ch5-notes/</link>
      <pubDate>Mon, 01 Apr 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1314/learning/2019-04-01-linear-algebra-done-right-ch5-notes/</guid>
      <description>&lt;h1 id=&#34;selected-exercises&#34;&gt;&#xA;Selected Exercises&#xA;&lt;a href=&#34;#selected-exercises&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h2 id=&#34;5a&#34;&gt;&#xA;5.A&#xA;&lt;a href=&#34;#5a&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;12.&lt;/strong&gt; Define $ T \in \mathcal L(\mathcal P_4(\mathbf{R})) $ by&lt;/p&gt;&#xA;&lt;p&gt;$$&#xA;(Tp)(x) = xp&amp;rsquo;(x)&#xA;$$&lt;/p&gt;&#xA;&lt;p&gt;for all $ x \in \mathbf{R} $. Find all eigenvalues and eigenvectors of $ T $.&lt;/p&gt;&#xA;&lt;p&gt;Observe that, if $ p = a_0 + a_1 x + a_2 x^2 + a_3 x^3 + a_4 x^4 $, then&#xA;$$&#xA;x p&amp;rsquo;(x) = a_1 x + 2 a_2 x^2 + 3 a_3 x^3 + 4 a_4 x^4.&#xA;$$&lt;/p&gt;</description>
    </item>
    <item>
      <title>Linear Algebra Done Right - Chapter 4</title>
      <link>http://localhost:1314/learning/2019-03-31-linear-algebra-done-right-ch4-notes/</link>
      <pubDate>Sun, 31 Mar 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1314/learning/2019-03-31-linear-algebra-done-right-ch4-notes/</guid>
      <description>&lt;h1 id=&#34;selected-exercises&#34;&gt;&#xA;Selected Exercises&#xA;&lt;a href=&#34;#selected-exercises&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;strong&gt;4.&lt;/strong&gt; Suppose $m$ and $n$ are positive integers with $ m \leq n $, and suppose $ \lambda_1, \dots, \lambda_m \in F $. Prove that there exists a polynomial $ p \in \mathcal P(\mathbf{F}) $ with $ \deg p = n $ such that $ 0 = p(\lambda_1) = \cdots = p(\lambda_m) $ and such that $ p $ has no other zeros.&lt;/p&gt;&#xA;&lt;p&gt;First, we can show that the polynomial $p&amp;rsquo;(z) = (z - \lambda_1) \cdots (z-\lambda_m) $ with $ \deg p&amp;rsquo; = m $ has $ 0 = p&amp;rsquo;(\lambda_1) = \cdots = p&amp;rsquo;(\lambda_m) $ and no other zeros. By 4.12, we know that $ p&amp;rsquo; $ has at most $ m $ zeros and therefore has no other zeros besides $ \lambda_1, \dots, \lambda_m $.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Linear Algebra Done Right - Chapter 3</title>
      <link>http://localhost:1314/learning/2019-03-30-linear-algebra-done-right-ch3-notes/</link>
      <pubDate>Sat, 30 Mar 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1314/learning/2019-03-30-linear-algebra-done-right-ch3-notes/</guid>
      <description>&lt;h1 id=&#34;selected-exercises&#34;&gt;&#xA;Selected Exercises&#xA;&lt;a href=&#34;#selected-exercises&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h2 id=&#34;3d&#34;&gt;&#xA;3.D&#xA;&lt;a href=&#34;#3d&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;7.&lt;/strong&gt; Suppose $ V $ &amp;amp; $ W $ are finite-dimensional. Let $ v \in V $. Let&#xA;$$&#xA;E = \{ T \in \mathcal L(V, W): T(v) = 0 \}.&#xA;$$&lt;/p&gt;&#xA;&lt;p&gt;(a)  Show that $ E $ is a subspace of $ \mathcal L(V, W) $.&lt;br&gt;&#xA;(b)  Suppose $ v \neq 0 $. What is $\dim E$?&lt;/p&gt;&#xA;&lt;p&gt;For (a), to show $ E $ is a subspace of $ \mathcal L(V, W) $, we need to show that $ E $ contains zero and is closed under both addition and multiplication.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Linear Algebra Done Right - Chapter 2</title>
      <link>http://localhost:1314/learning/2018-12-24-linear-algebra-done-right-ch2-notes/</link>
      <pubDate>Mon, 24 Dec 2018 00:00:00 +0000</pubDate>
      <guid>http://localhost:1314/learning/2018-12-24-linear-algebra-done-right-ch2-notes/</guid>
      <description>&lt;h1 id=&#34;intro&#34;&gt;&#xA;Intro&#xA;&lt;a href=&#34;#intro&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;I&amp;rsquo;ve recently been making my way through Axler&amp;rsquo;s &lt;em&gt;Linear Algebra Done Right&lt;/em&gt; and, as a way to motivate myself to continue, have decided to blog my notes and solutions for exercises as I go.&lt;/p&gt;&#xA;&lt;h1 id=&#34;insights&#34;&gt;&#xA;Insights&#xA;&lt;a href=&#34;#insights&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h2 id=&#34;section-2a&#34;&gt;&#xA;Section 2.A&#xA;&lt;a href=&#34;#section-2a&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h4 id=&#34;you-can-convert-any-linearly-dependent-list-to-a-linearly-independent-list-with-the-same-span&#34;&gt;&#xA;You can convert any linearly dependent list to a linearly independent list with the same span.&#xA;&lt;a href=&#34;#you-can-convert-any-linearly-dependent-list-to-a-linearly-independent-list-with-the-same-span&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h4&gt;&#xA;&lt;p&gt;By the linear dependence lemma, if you have a list that&amp;rsquo;s linearly dependenty, then you can remove one item without changing the list&amp;rsquo;s span. By inductive hand-waving, that means we can remove one item repeatedly while not changing the list&amp;rsquo;s span until the list becomes linearly independent. This seems like it might be useful by analogy to compression, if we assume the span captures some essential property of a list of vectors, we can remove items from a linearly dependent list without changing its final representation.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
