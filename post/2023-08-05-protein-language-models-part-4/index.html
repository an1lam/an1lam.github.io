<!DOCTYPE html>
<html lang="en-us"><head>
<title> - Stephen Malina</title>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover">
<meta name="description"
    content="
Scaling
#

Some of the earliest evidence for PLM scaling behaving similar to LLM scaling came from ESM-1b. The following plot shows model performance as a function of (log10) number of parameters. As seen there, they also trained LSTM models at two different sizes as baselines.


    
  
    ⊕
    
    


ESM-1b perplexity vs. log10(# of parameters) by model type on a held-out subset of UniRef50.







  
  
  







RITA provided even stronger evidence for PLMs following scaling laws. The RITA paper trained auto-regressive models of different sizes and used their performance to derive power laws for scaling performance. It used this power law to pick 4 ">
<link rel="canonical" href="https://stephenmalina.com/post/2023-08-05-protein-language-models-part-4/" />



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/modern-normalize/1.1.0/modern-normalize.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />



<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
<link rel="preload" as="style"
      href="https://fonts.googleapis.com/css?family=Noto+Serif+SC|Noto+Emoji&display=swap" />
<link rel="stylesheet"
      href="https://fonts.googleapis.com/css?family=Noto+Serif+SC|Noto+Emoji&display=swap"
      media="print" onload="this.media='all'" />
<noscript>
<link rel="stylesheet"
      href="https://fonts.googleapis.com/css?family=Noto+Serif+SC&display=swap" />
</noscript>


  
  
    
      
        <link rel="stylesheet" href="https://stephenmalina.com/css/hugo-tufte.min.501b334050bf0c6cb2483ec40debea9453e83d461ef681f15b5009d07c834d9d.css" integrity="sha256-UBszQFC/DGyySD7EDevqlFPoPUYe9oHxW1AJ0HyDTZ0=" crossorigin="anonymous">
      
    
  



  
  
    
      
        <link rel="stylesheet" href="https://stephenmalina.com/css/hugo-tufte-options.min.e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.css" integrity="sha256-47DEQpj8HBSa&#43;/TImW&#43;5JCeuQeRkm5NMpJWZG3hSuFU=" crossorigin="anonymous">
      
    
  


<link rel="stylesheet" href="https://stephenmalina.com/css/hugo-tufte-override.css">

</head>
<body>


<article id="main">
  <section>
<h1 class="content-title"></h1></section>

  

  <section><h2 id="scaling">
Scaling
<a href="#scaling" class="heading-anchor">#</a>
</h2>
<p>Some of the earliest evidence for PLM scaling behaving similar to LLM scaling came from ESM-1b. The following plot shows model performance as a function of (log10) number of parameters. As seen there, they also trained LSTM models at two different sizes as baselines.</p>


    <figure >
  
    <label for="marginfig-1" class="margin-toggle marginnote-ind">⊕</label>
    <input type="checkbox" id="marginfig-1" class="margin-toggle"/>
    <span class="marginnote">


ESM-1b perplexity vs. log10(# of parameters) by model type on a held-out subset of UniRef50.



</span>



  
  <img src="https://stephenmalina.com/images/esm1b-custom-scaling-1.png" alt="Image">
  




</figure>


<p><a href="https://paperswithcode.com/paper/rita-a-study-on-scaling-up-generative-protein">RITA</a> provided even stronger evidence for PLMs following <a href="https://arxiv.org/abs/2001.08361">scaling laws</a>. The RITA paper trained auto-regressive models of different sizes and used their performance to derive power laws for scaling performance. It used this power law to pick 4</p>
<p>As LLM papers often do, the ESM-1b paper used perplexity (exponent of the entropy) to evaluate the impact of model size on performance. Much of perplexity&rsquo;s value in NLP comes from its strong correlation with performance on a wide range of practical tasks. We don&rsquo;t have nearly as strong an understanding of the relationship between PLM perplexity and downstream task performance but preliminary evidence points to a relationship.</p>


    <figure class="fullwidth">
  
  
  <img src="https://stephenmalina.com/images/esm1v-fig9.gif" alt="Image">
  
  <figcaption>


ESM-1v Figure 9: Performance of different sized models on downstream tasks. We see a clear relationship between model size and downstream task performance. Combined with the results from ESM-1b on the relationship between model size and ECE, this transitively points to a strong relationship between LM loss and downstream task performance.




  </figcaption>






</figure>


<p>ProtTrans furnished additional evidence for a scaling relationship, in this case between the number of data points seen during training and performance on a different set of downstream tasks. Even if we ignore the data points for the three T5 models because of the size confounder, the trend of seeing more data points improving performance on the 3-state secondary structure classification task remains strong.</p>


    <figure >
  
    <label for="marginfig-3" class="margin-toggle marginnote-ind">⊕</label>
    <input type="checkbox" id="marginfig-3" class="margin-toggle"/>
    <span class="marginnote">


ProtTrans Figure 7: <code>$ \log_{10}(\textrm{Number of samples}) $</code> vs. performance on NEW364 secondary structure prediction. Even confounded by different model architectures, we see a high correlation between number of samples seen during training and downstream task performance.



</span>



  
  <img src="https://stephenmalina.com/images/prottrans-fig7.png" alt="Image">
  




</figure>


<ul>
<li>ESMFold</li>
<li><a href="https://arxiv.org/abs/2206.13517">Progen2</a>
<ul>
<li>Mixed evidence:
<ul>
<li>Use table 3 to make a zero-shot fitness prediction performance plot as a function of size</li>
<li>Use table 4 to make a downstream task performance plot as a function of size</li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
  <section><footer class="page-footer">
<hr />

<div class="previous-post" style="display:inline-block;">
  
  <a class="link-reverse" href="https://stephenmalina.com/subscribe/?ref=footer">« </a>
  
</div>

<div class="next-post", style="display:inline-block;float:right;">
  
  <a class="link-reverse" href="https://stephenmalina.com/home/?ref=footer"> »</a>
  
</div>

<ul class="page-footer-menu">
  
  
  <li><a href="https://twitter.com/an1lam">Twitter</a></li>
  
  
  

  
  <li><a href="https://github.com/an1lam">GitHub</a></li>
  

  

  

  

  

  

  

  

  

  

  
  <li><a href="https://scholar.google.com/citations?user=Q6_3PJEAAAAJ">Google Scholar</a></li>
  
  
  
    <li><a href="https://github.com/an1lam"><i class='fa fa-github fa-2x'></i>  </a></li>
  
    <li><a href="https://twitter.com/an1lam"><i class='fa fa-twitter fa-2x'></i>  </a></li>
  
    <li><a href="mailto:stephenmalina@gmail.com"><i class='fa fa-envelope fa-2x'></i>  </a></li>
  
    <li><a href="https://stackoverflow.com/users/1631855/an1lam"><i class='fa fa-stack-overflow fa-2x'></i>  </a></li>
  
</ul>


<p>
  Powered by <a href="https://gohugo.io">Hugo</a> and the
  <a href="https://github.com/loikein/hugo-tufte">Tufte theme</a>.
</p>




</footer>
</section>
  <section><nav class="menu">
    <ul>
    
        <li><a href="https://stephenmalina.com/">Home</a></li>
    
        <li><a href="https://stephenmalina.com/about">About</a></li>
    
        <li><a href="https://stephenmalina.com/post">Post</a></li>
    
        <li><a href="https://stephenmalina.com/bets">Bets</a></li>
    
        <li><a href="https://stephenmalina.com/fiction">Fiction</a></li>
    
        <li><a href="https://stephenmalina.com/greatest-hits">Greatest Hits</a></li>
    
        <li><a href="https://stephenmalina.com/tags">Tags</a></li>
    
    </ul>
</nav></section>
</article>





</body>

</html>
