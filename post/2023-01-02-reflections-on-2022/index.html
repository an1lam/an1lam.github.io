<!DOCTYPE html>
<html lang="en-us"><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1314&amp;path=livereload" data-no-instant defer></script>
<title>Reflections on 2022 - Stephen Malina</title>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover">
<meta name="description"
    content="2022 was another exciting, fun year. To round it out, I wrote up some scattered reflections. Enjoy, or if you aren&rsquo;t a fan of this genre of post, don&rsquo;t! Also, each section is mostly self-contained and they&rsquo;re not organized in any special order, so don&rsquo;t hesitate to skip a section if it sounds boring.

Writing
#

2022 felt like a productive year for blogging even though my post count dropped down from 11 in 2021 to 6. I&rsquo;m thrilled with how my and Austin&rsquo;s decentralization post came out. The topic&rsquo;s something I&rsquo;d thought about a lot but been unable to put into writing until Austin and I hashed it out together. Collaborating with Austin also reminded me how much I enjoy collaborating. It&rsquo;s also a strength of the medium and so I hope to do more of it in 2022. ">
<link rel="canonical" href="http://localhost:1314/post/2023-01-02-reflections-on-2022/" />



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/modern-normalize/1.1.0/modern-normalize.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />



<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
<link rel="preload" as="style"
      href="https://fonts.googleapis.com/css?family=Noto+Serif+SC|Noto+Emoji&display=swap" />
<link rel="stylesheet"
      href="https://fonts.googleapis.com/css?family=Noto+Serif+SC|Noto+Emoji&display=swap"
      media="print" onload="this.media='all'" />
<noscript>
<link rel="stylesheet"
      href="https://fonts.googleapis.com/css?family=Noto+Serif+SC&display=swap" />
</noscript>


  
  
    
      <link rel="stylesheet" href="http://localhost:1314/css/hugo-tufte.min.css">
    
  



  
  
    
      <link rel="stylesheet" href="http://localhost:1314/css/hugo-tufte-options.min.css">
    
  


<link rel="stylesheet" href="http://localhost:1314/css/hugo-tufte-override.css">

</head>
<body>


<article id="main">
  <section>
<h1 class="content-title">Reflections on 2022</h1></section>

  

  <section><p>2022 was another exciting, fun year. To round it out, I wrote up some scattered reflections. Enjoy, or if you aren&rsquo;t a fan of this genre of post, don&rsquo;t! Also, each section is mostly self-contained and they&rsquo;re not organized in any special order, so don&rsquo;t hesitate to skip a section if it sounds boring.</p>
<h2 id="writing">
Writing
<a href="#writing" class="heading-anchor">#</a>
</h2>
<p>2022 felt like a productive year for blogging even though my post count dropped down from 11 in 2021 to 6. I&rsquo;m thrilled with how my and Austin&rsquo;s <a href="https://stephenmalina.com/post/2022-11-21-decentralization-of-atoms-is-underrated/">decentralization post</a> came out. The topic&rsquo;s something I&rsquo;d thought about a lot but been unable to put into writing until Austin and I hashed it out together. Collaborating with Austin also reminded me how much I enjoy collaborating. It&rsquo;s also a strength of the medium and so I hope to do more of it in 2022.</p>
<p>My other favorite post from this past year is my <a href="https://stephenmalina.com/post/2022-04-24-book-review-egamu/">review</a> of <em>Elmer Gates and the Art of Mind-using</em>. While this post may appeal to a more niche audience, it helped me finally collect my thoughts about the book after years of mulling it over and struggling to wrap my head around it. The book&rsquo;s also obscure enough that I suspect my counterfactual impact in terms of exposing people to its ideas is high.</p>
<p>Looking ahead to 2023, this past month, I&rsquo;ve been have been <a href="https://searchcaster.xyz/search?merkleRoot=0x0201bb6bf0839255f0e1e99191af56cc9afa5c9d840256c65c09fd489d6e1bc2">rediscovering</a> the joy of <a href="https://searchcaster.xyz/search?merkleRoot=0xdd55229764b271cfa0603b7393abe8cc801775766a0d63f56026f056d8b26d52">dense</a> writing. My writing of the past few years has heavily skewed towards a Frankensteinian patchwork of the <a href="https://owl.purdue.edu/owl/general_writing/writing_style/plain_style%20.html">plain style</a> and a related style I associate with the <a href="https://astralcodexten.substack.com/">SlateStarCodex</a> sphere. While the Platonic ideal of this style (which my writing doesn&rsquo;t live up to) achieves valuable clarity and precision, my heart longs to produce writing that achieves aesthetic in addition to utilitarian goals. I&rsquo;m not yet sure what form my attempts at this will take, but I intend to do something.</p>
<p>Ironically, I&rsquo;ve also been feeling a desire to write more pedagogical material about some combination of ML and bio. While the internet is filled with ML tutorials, bio has much less of a culture of posting explanations on the internet. As a result, ML <strong>plus</strong> bio topics lack clear tutorials and even relatively basic bio methods often can be hard to find good information on. In contrast to exploring a different style of writing, I&rsquo;m not sure whether I&rsquo;ll actually act on this desire. Explaining things trades off more directly with trying to learn new things (unless I&rsquo;m explaining a thing I&rsquo;m learning, which isn&rsquo;t a bad idea) and the former is something I want to be more deliberate about this upcoming year.</p>
<p>Zooming out, I feel like the benefit of writing on the internet is massive (see Ben Kuhn&rsquo;s <a href="https://www.benkuhn.net/writing/">recent post</a>) but can take some time to manifest. For me, 2022 was definitely the year in which the promised benefit of my internet presence fully manifested. New friendships based on initial internet interactions solidified, collaborations happened, and ongoing correspondence led to new ideas. I&rsquo;m incredibly grateful for all these things and continue to think that, for all its issues, the internet is still underrated.</p>
<h2 id="work">
Work
<a href="#work" class="heading-anchor">#</a>
</h2>
<p>This was another great year for work. My job at Dyno continues to be my favorite job I&rsquo;ve had and I remain passionate about the mission and team. While most of my work isn&rsquo;t shared publicly, I played a small role in a <a href="https://arxiv.org/abs/2211.10422">paper</a> we published and presented on at both <a href="https://sites.google.com/cs.washington.edu/mlcb2022/?pli=1">MLCB</a> and the NeurIPS <a href="https://www.lmrl.org/">LMRL workshop</a>.</p>
<p>The biggest shift in the latter half of the year was moving from being an individual contributor to a manager. I feel green enough at it that I don&rsquo;t think I have novel things to say yet, although I hope to in the future. I&rsquo;m learning a ton but enjoying it quite a bit while experiencing some of the classic struggles (such as the desire to solve problems by coding).</p>
<p>At the company and industry level, reading business books, talking to people at similar companies, and observing our own challenges, have all strengthened my conviction that large chunks of software wisdom rely on an implicit presumption of fast feedback loops. Ideas like getting things in front of customers quickly and optimizing for cycle time may still apply in the world of therapeutics but at least require a lot of rejiggering to deal with the reality of biological experimentation. So far, my thinking about this has been scattered and led to me exploring ideas from other areas such as manufacturing (see section on reading and learning), but I also hope to write more about this in the future.</p>
<h2 id="ai">
AI
<a href="#ai" class="heading-anchor">#</a>
</h2>
<p>I&rsquo;ve been working in ML (for biology) for over 3 years now and had kept track of work on <a href="https://www.gwern.net/Scaling-hypothesis">scaling laws</a> and <a href="https://arxiv.org/abs/2005.14165">other</a> <a href="https://proceedings.neurips.cc/paper/2020/file/1f89885d556929e98d3ef9b86448f951-Paper.pdf">signs</a> <a href="https://openai.com/blog/ai-and-compute/">that</a> that suggested AI progress wasn&rsquo;t slowing since ~2020, yet I have still been surprised by AI progress this year. The capabilities that combining pre-trained language models with fine-tuning unlocked shocked me, in particular for areas like math, where <a href="https://arxiv.org/abs/2103.03874">early signs</a> suggested scaling might provide less of an advantage, but <a href="https://ai.googleblog.com/2022/06/minerva-solving-quantitative-reasoning.html">subsequent results</a> suggest the right fine-tuning dataset combined with prompting techniques unlock a scaling advantage. In image generation, where I&rsquo;d been paying less attention, the rapid transition of image generation models from amusing toy to potentially transformative tool snuck up on me.</p>
<p>Something I haven&rsquo;t seen discussed as much is how the second half of the year&rsquo;s progress has been a triumph for innovation in the <a href="https://www.humanprogress.org/ridley-how-innovation-works/">Matt Ridley sense</a>. As more models have transitioned from research artifact to product, many discoveries about how to deploy them have come from outside the traditionally research community. On the LLM side of things, while credit attribution often hides or minimizes this, many prompting improvements either originated or were refined on Twitter by people like <a href="https://twitter.com/goodside">Riley Goodside</a> and then only later packaged into papers or other legible artifacts. I expect this trend to continue as companies built on LLMs refine prompting strategies in the <a href="https://twitter.com/goodside/status/1608715122419302401">gauntlet</a> of the consumer marketplace, although many of these hardwon learnings will likely remain secret. On the image generation side, while all three of DALL-E 2, Midjourney, and Stable Diffusion&rsquo;s enabled an explosion of public experimentation with prompting tricks, only Stable Diffusion allowed for <a href="https://huggingface.co/docs/diffusers/training/dreambooth">tool-building</a> on top of it.</p>
<p>In terms of my own state of mind on macro questions around AI, like anyone who&rsquo;s not bullshitting, I don&rsquo;t know what&rsquo;s going to happen. I&rsquo;m unconvinced by the harshest critics who declare that deep learning is fundamentally flawed and doomed to <a href="https://nautil.us/deep-learning-is-hitting-a-wall-238440/">hit a wall</a> but not yet convinced AI is on the <a href="https://twitter.com/mobav0/status/1566821953230364672">precipice</a> of <a href="https://twitter.com/sama/status/1436028668082462748?s=20&amp;t=6P_4KkoJUEPxPhiVBbqr5Q">catalyzing</a> rapid economic change on the scale of the industrial revolution. My views also shift around depending on who I&rsquo;ve read or talked to in the past week or which new results just came out. So, consider the rest of this a snapshot of rapidly evolving views. I expect my views to have changed dramatically and hopefully solidified in the next 6 months and certainly in the next year.</p>
<p>I&rsquo;m unconvinced by the critics because I&rsquo;ve observed them move the goalposts as large language models continue to trample some of the previously pointed to benchmarks (such as Winogrande). I&rsquo;m also just not convinced by their arguments. While I suspect LLMs on their own still lack some key ingredients of &ldquo;general&rdquo; intelligence, any ingredient I can point to has multiple groups of researchers actively working on it. For example, I pointed to long term memory and continual learning as capabilities to watch out for in my <a href="https://stephenmalina.com/post/2022-09-17-ai-omens/">omens post</a>. Both of these areas have had significant active work in the past year (<a href="https://arxiv.org/abs/2203.08913">Memorizing Transformers</a> as an example of the first) and are discussed as areas for improvement by the community. On top of this, seeing how pre-trained models enable reinforcement learning both for <a href="https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf">language generation</a> and open world game tasks (<a href="https://openai.com/blog/vpt/">VPT</a>, <a href="https://developer.nvidia.com/blog/building-generally-capable-ai-agents-with-minedojo/">MineDojo</a>, <a href="https://www.deepmind.com/blog/building-interactive-agents-in-video-game-worlds">Building interactive agents in open world</a>), I have a spidey sense that scaling combined with architectural innovation may have been more of a bottleneck than some of these other steps towards enabling breakthrough applications.</p>
<p>On the other hand, while AI developments are already improving translation, <a href="https://www.theverge.com/2021/6/10/22527476/google-machine-learning-chip-design-tpu-floorplanning">chip</a> <a href="https://developer.nvidia.com/blog/designing-arithmetic-circuits-with-deep-reinforcement-learning/">design</a>, <a href="https://www.deepmind.com/publications/eta-prediction-with-graph-neural-networks-in-google-maps">ETA prediction</a>, and many other areas beyond language, I&rsquo;m still not yet sure how much the AI developments we&rsquo;re seeing will translate to <a href="https://twitter.com/cameronfen1/status/1567205153937457153">broad accelerating economic growth</a> in the next 5-10 years. Like Mulder in <a href="https://en.wikipedia.org/wiki/The_X-Files:_I_Want_to_Believe">The X-Files</a>, I want to believe, but seeing how previously heralded technological breakthroughs failed to achieve the promised level of transformative impact and reading things like <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3496364">Testing the Automation Revolution Hypothesis</a> have made me wary of prematurely assuming impressive looking technologies translate to real world impact. Especially when it comes to impact in the world of bits not atoms, I have the sense that regulatory barriers combined with <a href="http://johnsalvatier.org/blog/2017/reality-has-a-surprising-amount-of-detail">reality&rsquo;s surprising detail</a> place a speed limit on how quickly AI can transform industries.</p>
<p>Playing my own devil&rsquo;s advocate, one thing that I can see accelerating economic growth would be continued progress in robotics. Broadly applicable, easy-to-steer robotics seem special to me because so much of the economy appears to bottom out with humans understanding commands <strong>and</strong> doing some amount of physical work. The messy interaction between these two things &ndash; having to interact and deploy physical dexterity</p>
<p>&ndash; has made me skeptical of rapid robotic deployment in the past, but seeing like <a href="https://sites.research.google/palm-saycan">PaLM-SayCan</a>, <a href="https://twitter.com/hausman_k/status/1602720768965898240">RT-1</a>, and <a href="https://vimalabs.github.io/">VIMA</a> enable robotics to ride on the coattails of scale and LLM improvements has me wondering if robotic workers may come sooner than I previously expected. Even with this though, I put some weight on the views of people like Rodney Brooks who emphasize the gap between impressive demos like <a href="https://en.wikipedia.org/wiki/SHRDLU">SHRDLU</a> and real world deployment.</p>
<p>Of course, the possibility of <a href="https://en.wikipedia.org/wiki/Artificial_general_intelligence">artificial general intelligence</a> (AGI), transformative AI, or <a href="https://www.cold-takes.com/transformative-ai-timelines-part-1-of-4-what-kind-of-ai/">PASTA</a> looms over all of this like a spectre. To this point, much of my discussion has assumed that AI will behave like a transformative, albeit &ldquo;normal&rdquo; technology (such as electricity or mechanization) to which we can apply relatively standard economic erasoning. All this goes out the window if you assume we&rsquo;re going to develop AI that exceeds human intelligence and/or multiplies the amount of human-level intelligence by a factor of 10-1000. In that world, the entire economy could change overnight as entirely new fields of science and technology emerge at the speed of melting GPUs or it could be our doom. My thinking on this topic is even more confused than on the rest of this so I&rsquo;m going to declare &ldquo;<a href="https://en.wikipedia.org/wiki/Mu_(negative)">Mu</a>&rdquo; here.</p>
<p>Jumping back to the more personal and related to the above, part of me definitely is drawn to the areas in which AI progress seems to only be limited by the number of GPUs. One of the challenges of working with the biological world is that experiments take time. Given that, I can&rsquo;t help but occasionally feel jealous seeing how people working with language models and image generation can iterate on prompts, models, and other things at the speed of code. On the other hand, I remind myself that this exact psychological advantage the world of pure software has may (partially since money was also a factor) explain why for so long tech failed to influence the world of atoms and that <a href="https://a16z.com/2020/04/18/its-time-to-build/">building</a> requires dealing with the messiness of the real world (at least for now).</p>
<p>Going a bit more meta, I&rsquo;ve also been thinking about how to continue refining my views on AI. One fear I have is falling prey to the trap Gwern describes <a href="https://www.gwern.net/Scaling-hypothesis#critiquing-the-critics">here</a>. While I think I avoid the false certainty described there, I am definitely prone to the status quo adaptation he describes. (&ldquo;The voice utters simple arguments about why the status quo will prevail, and considers only how the wild new idea could fail (and not all the possible options).&rdquo;) I can already feel the sense of surprise slowly fading as I quickly adapt to the <a href="https://twitter.com/bentossell/status/1608462233536974849">new normal</a> in which an <a href="https://github.com/features/copilot">AI assistant helps me code better</a> and <a href="https://chat.openai.com/chat">can help me rewrite text or find answers to questions</a>. To try and combat this pundit-style fading of surprise and goal post shifting, I&rsquo;ve been aggressively capturing my <a href="https://stephenmalina.com/post/2022-09-17-ai-omens/">thoughts</a> and predictions on my <a href="https://stephenmalina.com/post/2022-11-16-gpt4-predictions/">blog</a> and on <a href="https://manifold.markets/StephenMalina?tab=portfolio">Manifold</a>. Most of these predictions will resolve in the future, but by the end of 2024, but I expect to have a clearer sense of whether my current expectations around the pace of progress are calibrated. However, I worry that making predictions and occasionally writing about it isn&rsquo;t enough to keep me evolving my view on AI as things develop. If any readers have thoughts on other things I can/should be doing, I&rsquo;d love to hear them.</p>
<p>Wrapping up this section, I feel incredibly grateful to be (broadly) part of a field that&rsquo;s clearly going through a scientific and technological renaissance. In addition to the ML researchers who came before, I&rsquo;m especially grateful to the unsung heros who toiled on maintaining Moore&rsquo;s Law for long enough to make all this possible. I look forward to seeing what the next year(s) bring.</p>
<h2 id="learning-and-reading">
Learning and reading
<a href="#learning-and-reading" class="heading-anchor">#</a>
</h2>
<p>I failed to stick with my plans for structured technical learning in 2022. A storm of faltering discipline, work competing for energy (I use &ldquo;energy&rdquo; not &ldquo;time&rdquo; for a reason), and getting distracted by AI progress/current events/drama, my technical learning efforts followed a consistent pattern of starting a textbook or course and then stalling out a few chapters/lectures in. The one counter-example to this was <strong>Immune</strong>, which I highly recommend but borders on popular science in its readability. One takeaway from these repeated failures is that it&rsquo;s better to make slow consistent progress vs. set unrealistic expectations and then fail to achieve them at all. I&rsquo;d rather have made it through my entire plan for learning reinforcement learning, intermediate cell biology, organic chemistry vs. the 20-30% of each I ended up going through.</p>
<p>On the flip side, I enjoyed reading about management and strategy much more than I expected. While I&rsquo;m wary of business books in general, I got lucky with the books people I either found or had recommended to me. On the management side, I read <a href="https://www.goodreads.com/en/book/show/324750">High Output Management</a> and <a href="https://www.goodreads.com/book/show/45303387-an-elegant-puzzle?from_search=true&amp;from_srp=true&amp;qid=J5cbdole9r&amp;rank=1">An Elegant Puzzle</a>.</p>
<p>For a book written before email was widespread, <em>High Output Management</em> impressed me with its relevance. Grove&rsquo;s writing and thinking has a groundedness that many other business authors&rsquo; lacks, I suspect related to his thinking being clearly connected to his observations in the trenches at Intel. When Grove tells a story about Cindy who&rsquo;s balancing the needs of her functional and business vertical manager, you can tell that while the names may be changed, the example is both real and not dumbed down. Grove also balances an analytic perspective with a real understanding of human behavior and incentives better than other management authors I&rsquo;ve read without falling prey to idealistic or ideological thinking.</p>
<p>Relative to <em>High Output Management,</em> <em>An Elegant Puzzle</em> had more relevant things to say about the nuts &amp; bolts of managing software teams, which I found helpful for getting a sense of best practices beyond what I&rsquo;ve seen firsthand and heard about from friends. Will Larson, the author, shares Grove&rsquo;s pragmatism and understanding of trade-offs, which shines through  in his discussion of challenges managers face. Related to my point in the work section, reading this book fed into my realization that software wisdom  feels ecologically adapted to environments with fast feedback loops from reality and the ability to draw clear abstraction boundaries. This means that figuring out which and how I can adapt ideas from this book to my own experiences requires creativity but realistically that&rsquo;s always partially true with advice anyway.</p>
<p>On the strategy side, I read <a href="https://www.goodreads.com/book/show/11721966-good-strategy-bad-strategy?ac=1&amp;from_search=true&amp;qid=0pIWf8fA8c&amp;rank=1">Good Strategy, Bad Strategy</a> at the recommendation of multiple coworkers and reread <a href="https://www.goodreads.com/book/show/53138083-working-backwards?ac=1&amp;from_search=true&amp;qid=j7uC7VX71L&amp;rank=1">Working Backwards</a> on a whim. <em>Good Strategy, Bad Strategy</em> crystallized the idea of strategy for me. This is self-indulgent but it feels a bit like a more thoughtful version of <a href="https://guzey.com/ideas-not-mattering-is-a-psyop/">Ideas not mattering is a psyop</a> with actual evidence backing it up. I have had notes for a review sitting around for a few months now but haven&rsquo;t gotten around to converting them to prose. If you&rsquo;re interested in them, I&rsquo;m happy to share them in their current form.</p>
<p><em>Working Backwards</em> seems like one of the best books in the genre of &ldquo;this is how a company actually functions with some but minimal sugar-coating or hindsight bias.&rdquo; This is an amazing genre and I wish there were way more books like this. That said, it&rsquo;s important to recognize that this genre is not the same as the hypothetical genre of &ldquo;these are all good ideas for your company.&rdquo; I don&rsquo;t feel the need to write a review of this book because Cedric Chin has already written a <a href="https://commoncog.com/working-backwards/">better one</a> than I could.</p>
<p>Circling back on technical stuff, while my structured efforts failed, I still managed to learn a lot via more haphazard means such as discussions with coworkers and friends, journal clubs, and Twitter/Arxiv trolling. Through a slow process of accretion and mystical background subconscious mental jigsaw puzzle pieces falling into place, I feel much better about my ML knowledge than I did a year ago in terms of my ability to understand the core of results in different areas. As an example, it took me a <strong>long</strong> time to really grok what the hell probabilistic ML people were talking about but I now feel like I can generally follow when people start going on about posteriors in the context of not originally probabilistic methods (such as neural networks). One obvious lesson I seemingly need to have hammered into me again and again here is that learning can be a lot more pleasant when it involves other people even if that comes at the expense of some structure. I&rsquo;d like to find a way to integrate this insight into more structured learning efforts in 2023 if I can.</p>
<p>On the leisure reading side, I&rsquo;m no longer embarrassed to admit that a highlight of my year was discovering <a href="https://www.reddit.com/r/ProgressionFantasy/">progression fantasy</a>. The genre has a lot of crap but pushes all my buttons by focusing on characters who continuously grow. My two favorites that I&rsquo;ve read so far are the <a href="https://www.goodreads.com/series/192821-cradle">Cradle series</a> by Will Wight (who is also a writing machine) and <a href="https://www.goodreads.com/book/show/60236959-mother-of-learning?from_search=true&amp;from_srp=true&amp;qid=kTv86Gr5fU&amp;rank=4">Mother of Learning</a>. Azalea Ellis&rsquo;s <a href="https://www.azaleaellis.com/a-practical-guide-to-sorcery/">A Practical Guide to Sorcery</a> is a close second but has the advantage of chapter releases on a weekly cadence if you support <a href="https://www.patreon.com/azaleaellis/posts">Ellis on Patreon</a> like I do. I&rsquo;ve been disappointed by some of the other books I&rsquo;ve tried in the genre such as <a href="https://www.royalroad.com/fiction/41330/virtuous-sons-a-greco-roman-xianxia">Virtuous Sons</a>, so I&rsquo;d definitely recommended trying before you buy if you dip your toes in.</p>
<p>As always, I read a fair bit of science fiction this year. In the past week, I&rsquo;ve <a href="https://twitter.com/an1lam/status/1607757796098834440">received</a> some great recommendations from Twitter, which I started over the break. Other highlights include the hokey but memetically fit for my exact brain <a href="https://www.goodreads.com/book/show/59439117-upgrade">Upgrade</a>, <a href="https://www.goodreads.com/book/show/6606324-singularity-sky">Singularity Sky</a> and <a href="https://www.goodreads.com/book/show/6057293-iron-sunrise">Iron Sunrise</a> by the consistently excellent Charles Stross, and <a href="https://www.goodreads.com/book/show/50708450-house-of-suns">House of Suns</a> by Alastair Reynolds (hat tip: <a href="https://twitter.com/david_a_knowles/status/1561910067800285184?s=20&amp;t=lP3KhWcFbZo8kLTZAwRDNw">Twitter again</a>).</p>
<p>To close this section, a harder-to-place urge I&rsquo;ve been feeling over the past month &ndash; maybe related to reading David Goggins&rsquo;s <a href="https://www.goodreads.com/en/book/show/63079845#:~:text=In%20Never%20Finished%2C%20Goggins%20takes,quest%20for%20greatness%20is%20unending.">new book</a> &ndash; is the desire to conquer more concrete challenges in 2023. This may mean going through a hard course or textbook, working on a more defined side project, or something else. I&rsquo;m not sure, but if you have ideas or interest, let me know!</p>
<h2 id="predictions">
Predictions
<a href="#predictions" class="heading-anchor">#</a>
</h2>
<p>As discussed in my 2021 predictions post, I decided not to make annual predictions in 2022. In hindsight, I made the right decision to instead focus on making more predictions on <a href="https://www.metaculus.com/questions/">Metaculus</a> and <a href="manifold.markets">Manifold</a>. I ended up spending most of my time on the latter, making what looks like &gt;100 predictions over the course of the year. Based on profit alone, I&rsquo;m happy with the result given that I took a major loss on a not that representative doubled down bet about how much charity people would donate and had to climb my way back. In line with the post, I tried to focus my prediction efforts on areas like AI, general tech, and longevity where I have more inside knowledge. I&rsquo;ve been happy with this decision both because I&rsquo;ve had more predictive success here and because I think it has and will continue to help me adjust my views on AI more rapidly as things evolve.</p>
<p>I&rsquo;ve also enjoyed <a href="https://manifold.markets/StephenMalina?tab=markets">creating new markets</a> on Manifold and letting others bet on them. Creating precise but still interesting questions is surprisingly hard and some of my markets (<a href="https://manifold.markets/StephenMalina/will-someone-train-a-1t-parameter-d">example</a>) had some helpful discussion pointing out resolution criteria ambiguity.</p>
<p>Focusing more on Manifold did come with the downside of me making many low effort intuitive predictions but fewer predictions where I put effort into building an actual model. I&rsquo;m not sure 1) how much I should care about the latter type and 2) whether this is a me problem, the incentive gradient of Manifold, or both. Regardless, in 2023 I may try to push myself to hit some minimum budget of higher effort markets/predictions.</p>
<h2 id="indulgent-musings">
Indulgent musings
<a href="#indulgent-musings" class="heading-anchor">#</a>
</h2>
<p>I have a sense that in 2022 I settled into or maybe came to terms with certain parts of myself I will or won&rsquo;t try to change in the future and feel more equipped to focus on the former vs. agonizing over the latter. This may not be obvious from what I&rsquo;ve written above but feels entangled with a lot of it from my perspective. Accepting my desire to take on more ambitious learning challenges but not letting it mislead me towards idealism. Moving into a management role vs. previously obsessing over becoming less technical. These both have subtle but real ties to previous personality knots that now feel less tangled.</p>
<p>On a totally different note, although it stresses me often, I feel blessed to live in interesting times. I remember becoming disillusioned during college realizing that all the exciting developments I&rsquo;d read about on <a href="https://phys.org/">Phys Org</a> and <a href="https://singularityhub.com/">Singularity Hub</a> didn&rsquo;t seem to be having much of an impact on tech I actually used. Seeing the deployment of novel vaccines, exciting research and investment into reversing aging, ML, cheap solar panels, and much more makes the younger version of me feel warm fuzzies. While the COVID pandemic (obviously) came with devastating downsides, it partially catalyzed a bipartisan movement around <a href="https://www.theatlantic.com/ideas/archive/2022/01/scarcity-crisis-college-housing-health-care/621221/">abundance</a> <a href="https://www.bostonreview.net/forum_response/the-abundance-agenda/">and</a> <a href="https://www.theatlantic.com/science/archive/2019/07/we-need-new-science-progress/594946/">progress</a> which I&rsquo;m very excited about. Of course, I wish the above didn&rsquo;t come with war and an increasingly tense geopolitical situation, even there I find pockets of things to be optimistic about.</p>
<p>Here&rsquo;s to another year of doing, thinking, learning, screwing up, and having fun with all of it! I know it&rsquo;s popular to hate on the internet and social media but I for one look forward to another year online.</p>
</section>
  <section><footer class="page-footer">
<hr />

<div class="previous-post" style="display:inline-block;">
  
  <a class="link-reverse" href="http://localhost:1314/post/2022-11-21-decentralization-of-atoms-is-underrated/?ref=footer">« Decentralization of Atoms is Underrated</a>
  
</div>

<div class="next-post", style="display:inline-block;float:right;">
  
  <a class="link-reverse" href="http://localhost:1314/post/2023-01-11-viriditas-dialogue/?ref=footer">Dialogue on Viriditas »</a>
  
</div>

<ul class="page-footer-menu">
  
  
  <li><a href="https://twitter.com/an1lam">Twitter</a></li>
  
  
  

  
  <li><a href="https://github.com/an1lam">GitHub</a></li>
  

  

  

  

  

  

  

  

  

  

  
  <li><a href="https://scholar.google.com/citations?user=Q6_3PJEAAAAJ">Google Scholar</a></li>
  
  
  
    <li><a href="https://github.com/an1lam"><i class='fa fa-github fa-2x'></i>  </a></li>
  
    <li><a href="https://twitter.com/an1lam"><i class='fa fa-twitter fa-2x'></i>  </a></li>
  
    <li><a href="mailto:stephenmalina@gmail.com"><i class='fa fa-envelope fa-2x'></i>  </a></li>
  
    <li><a href="https://stackoverflow.com/users/1631855/an1lam"><i class='fa fa-stack-overflow fa-2x'></i>  </a></li>
  
</ul>


<p>
  Powered by <a href="https://gohugo.io">Hugo</a> and the
  <a href="https://github.com/loikein/hugo-tufte">Tufte theme</a>.
</p>




</footer>
</section>
  <section><nav class="menu">
    <ul>
    
        <li><a href="http://localhost:1314/">Home</a></li>
    
        <li><a href="http://localhost:1314/about">About</a></li>
    
        <li><a href="http://localhost:1314/post">Post</a></li>
    
        <li><a href="http://localhost:1314/bets">Bets</a></li>
    
        <li><a href="http://localhost:1314/greatest-hits">Greatest Hits</a></li>
    
        <li><a href="http://localhost:1314/tags">Tags</a></li>
    
    </ul>
</nav>
</section>
</article>





</body>

</html>
