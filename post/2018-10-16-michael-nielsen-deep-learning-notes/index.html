<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

<head>
<title>Stephen Malina - Notes on Michael Nielsen&#39;s &#34;Neural Networks and Deep Learning&#34;</title>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="">
<meta name="keywords" content="">
<meta name="author" content="Stephen Malina">
<meta name="generator" content="Hugo 0.68.3" />

  
  






<link rel="stylesheet" href="https://unpkg.com/purecss@2.0.3/build/pure-min.css">


    <link rel="stylesheet" href="https://unpkg.com/purecss@2.0.3/build/grids-responsive-min.css">








<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">


<link rel="stylesheet" href="https://stephenmalina.com/css/tufte.css">
<link rel="stylesheet" href="https://stephenmalina.com/css/hugo-tufte.css">
<link rel="stylesheet" href="https://stephenmalina.com/css/hugo-tufte-override.css">

  
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-187780632-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

  
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-187780632-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

</head>


<body>
<div id="layout" class="pure-g">
<article class="pure-u-1">
<header class="brand">
  <h1>Stephen Malina</h1>
  <h2>This is my blog. There are many others like it but this one is mine.</h2>
  <nav class="menu">
    <ul>
    
        <li><a href="https://stephenmalina.com/">Home</a></li>
    
        <li><a href="https://stephenmalina.com/about">About</a></li>
    
        <li><a href="https://stephenmalina.com/post">Post</a></li>
    
        <li><a href="https://stephenmalina.com/subscribe">Subscribe</a></li>
    
        <li><a href="https://stephenmalina.com/tags">Tags</a></li>
    
    </ul>
</nav>

</header>

<section>
<h1 class="content-title">
  
  <a href="https://stephenmalina.com/post/2018-10-16-michael-nielsen-deep-learning-notes/">Notes on Michael Nielsen&#39;s &#34;Neural Networks and Deep Learning&#34; :: Draft </a>
  
</h1>



  <span class="content-meta">
    


    
      <i class="fa fa-calendar"></i>
      &nbsp;Oct 16, 2018
    

    
      &nbsp;<i class="fa fa-clock-o"></i>
      &nbsp;6 min read
    

    
  </span>


</section>



<section><h1 id="chapter-1">Chapter 1</h1>
<h2 id="notes">Notes</h2>
<ul>
<li>
<p>How far can perceptron&rsquo;s take us? How much do Marvin Minsky&rsquo;s objections to perceptrons apply to sigmoid neurons?</p>
</li>
<li>
<p>[For QC]: How would I, if I were inventing logistic neurons for the first time, know to propose the sigmoid function? What properties would I look at?</p>
</li>
<li>
<p>[For QC]: How much do you rely on reasoning in your head vs. writing things out, especially when you&rsquo;re reading?</p>
</li>
<li>
<p>[For QC]: Ratio of thinking to reading, how much time do you spend on each point?</p>
</li>
<li>
<p>If we start with,</p>
<p>\begin{eqnarray}
\mbox{output} = \frac{1}{1+\exp(-\sum_j w_j x_j-b)}.
\end{eqnarray}</p>
<p>how do we end up with</p>
<p>\begin{eqnarray}
\Delta \mbox{output} \approx \sum_j \frac{\partial , \mbox{output}}{\partial w_j}
\Delta w_j + \frac{\partial , \mbox{output}}{\partial b} \Delta b,
\end{eqnarray}</p>
<p>? It makes intuitive sense that the change in output would be the change in weight times the partial derivative of output over weight, since this represents the amount output would change given a minute perturbation of weight, but I wouldn&rsquo;t know how to derive this.</p>
</li>
<li>
<p>The gradient is the transpose of the partial derivative of a vector, i.e.,</p>
<p>\begin{eqnarray}
\nabla C \equiv \left( \frac{\partial C}{\partial v_1},
\frac{\partial C}{\partial v_2} \right)^T.
\end{eqnarray}</p>
</li>
<li>
<p>Why does the change in $C$ equal,</p>
<p>\begin{eqnarray}
\Delta C \approx \frac{\partial C}{\partial v_1} \Delta v_1 +
\frac{\partial C}{\partial v_2} \Delta v_2?
\end{eqnarray}</p>
</li>
</ul>
<h2 id="exercises">Exercises</h2>
<p><strong>Sigmoid neurons simulating perceptrons, part I</strong><br>
Suppose we take all the weights and biases in a network of perceptrons, and multiply them by a positive constant, $c&gt;0$. Show that the behaviour of the network doesn&rsquo;t change.</p>
<p>Let&rsquo;s start with a single perceptron. How will its behavior change?</p>
<p>A perceptron&rsquo;s output function is,</p>
<p>\begin{eqnarray}
\mbox{output} = \left\{
\begin{array}{ll}
0 &amp; \mbox{if } w\cdot x + b \leq 0 \\\<br>
1 &amp; \mbox{if } w\cdot x + b &gt; 0
\end{array}
\right.
\tag{3}\end{eqnarray}</p>
<p>The same perceptron with its weights and biases multiplied by a positive constant will have output function,</p>
<p>\begin{eqnarray}
\mbox{output} = \left\{
\begin{array}{ll}
0 &amp; \mbox{if } c (w\cdot x) + c \cdot b \leq 0 \\\<br>
1 &amp; \mbox{if } c (w\cdot x) + c \cdot b &gt; 0
\end{array}
\right.
\tag{3}\end{eqnarray}</p>
<p>If we then pull the $c$ out of the equation further,</p>
<p>\begin{eqnarray}
\mbox{output} = \left\{
\begin{array}{ll}
0 &amp; \mbox{if } c (w\cdot x + b) \leq 0 \\\<br>
1 &amp; \mbox{if } c (w\cdot x + b) &gt; 0
\end{array}
\right.
\tag{3}\end{eqnarray}</p>
<p>Then we see that the function result being compared to 0 will be multiplied by a constant factor. Numbers less than 0, when multiplied by constant factors, are always still less than 0 and positive numbers, multiplied by constant factors, are always still greater than 0. Multiplying weights and biases by $c$ has no impact on the output of a given perceptron. Multiplying all perceptrons&rsquo; weights and biases by $c$ will also have no impact on a network&rsquo;s final output.</p>
<p><strong>Sigmoid neurons simulating perceptrons, part II</strong>
Suppose we have the same setup as the last problem - a network of perceptrons. Suppose also that the overall input to the network of perceptrons has been chosen. We won&rsquo;t need the actual input value, we just need the input to have been fixed. Suppose the weights and biases are such that $w \cdot x + b \neq 0 $ for the input $x$ to any particular perceptron in the network. Now replace all the perceptrons in the network by sigmoid neurons, and multiply the weights and biases by a positive constant c&gt;0. Show that in the limit as c→∞ the behaviour of this network of sigmoid neurons is exactly the same as the network of perceptrons. How can this fail when w⋅x+b=0 for one of the perceptrons?</p>
<p>In the case where $ w \cdot x + b \neq 0 $, $ c ( w \cdot x + b ) $ will approach $ \infty $ if $ w \cdot x + b &gt; 0 $ and $ - \infty $ if $ w \cdot x + b &lt; 0 $, giving us the same behavior we&rsquo;d expect from a perceptron because $ \sigma(-\infty) = 0$ and $ \sigma(\infty) = 1$. However, in the case where $ w \cdot x + b = 0 $, $ \sigma(0) = \frac{1}{2} $, which does not match the perceptron&rsquo;s output of 0.</p>
<p><strong>Bitwise representation of a digit</strong>
There is a way of determining the bitwise representation of a digit by adding an extra layer to the three-layer network above. The extra layer converts the output from the previous layer into a binary representation, as illustrated in the figure below. Find a set of weights and biases for the new output layer. Assume that the first 3 layers of neurons are such that the correct output in the third layer (i.e., the old output layer) has activation at least 0.99, and incorrect outputs have activation less than 0.01.
<img src="http://neuralnetworksanddeeplearning.com/images/tikz13.png" alt="Four layer network" /></p>
<p>The key to solving this problem is realizing that each output node should output an above-threshold response if the number indicated by the prior layer&rsquo;s output includes a 1 for this bit in its binary representation. For example, if layer 3 outputs 6, then nodes 1 (zero-indexed) and 2 should have outputs $&gt;0.99$. The following table includes the full mapping of layer 4 node activations to layer 3 non-0 inputs.</p>
<table>
<thead>
<tr>
<th align="left">Layer 4 Index</th>
<th align="left">Layer 3 Indices</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">0</td>
<td align="left">1, 3, 5, 7, 9</td>
</tr>
<tr>
<td align="left">1</td>
<td align="left">2, 3, 6, 7</td>
</tr>
<tr>
<td align="left">2</td>
<td align="left">4, 5, 6, 7</td>
</tr>
<tr>
<td align="left">3</td>
<td align="left">8, 9</td>
</tr>
</tbody>
</table>
<p>Note that 0 will automatically be represented as all &lt;0.01 activations.</p>
<p>From here, we have to construct weights that replicate an OR function of each output node&rsquo;s inputs. To make things simple, assume we have an activation threshold of 0.5, meaning $w \cdot x + b &gt; 0$ in order for a neuron to output an above-threshold activation. Weights will be a 4x10 dimensional vector, where output will be a 4x1 dimensional vector with each row equal to the dot product of $w[i]$ and $x[i]$.</p>
<p>Given this, we can have weights,
$$
\begin{bmatrix}
0 &amp; 2 &amp; 0 &amp; 2 &amp; 0 &amp; 2 &amp; 0 &amp; 2 &amp; 0 &amp; 2 \\\<br>
0 &amp; 0 &amp; 2 &amp; 2 &amp; 0 &amp; 0 &amp; 2 &amp; 2 &amp; 0 &amp; 0 \\\<br>
0 &amp; 0 &amp; 0 &amp; 0 &amp; 2 &amp; 2 &amp; 2 &amp; 2 &amp; 0 &amp; 0 \\\<br>
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 2 &amp; 2 \\\<br>
\end{bmatrix}
$$</p>
<p>and a bias of -1 for all inputs. This way, each output neuron will have $w \cdot x + b &gt; 0$ when any of the layer 3 inputs to which it maps in the above table have activation $0.99$ because $ w \cdot x + b = 2 * 0.99 - 1 = .98 $, but $ w \cdot x + b &lt; 0$ when all mapped inputs to the neuron are $ \leq 0.01 $, because $ 2 * 0.01 - 1 = -.98 $.</p>
</section>
<section>
  

  <footer class="page-footer">
		<hr>
		<ul class="page-footer-menu">
		
      <li><a href="https://github.com/an1lam"><i class='fa fa-github fa-2x'></i> </a></li>
		
      <li><a href="https://twitter.com/an1lam"><i class='fa fa-twitter fa-2x'></i> </a></li>
		
      <li><a href="mailto:stephenmalina@gmail.com"><i class='fa fa-envelope fa-2x'></i> </a></li>
		
      <li><a href="https://stackoverflow.com/users/1631855/an1lam"><i class='fa fa-stack-overflow fa-2x'></i> </a></li>
		
		</ul>

  
    <p>
      Powered by <a href="https://gohugo.io">Hugo</a> and the
      <a href="https://github.com/shawnohare/hugo-tufte">Tufte theme</a>.
    </p>
  

	<div class="copyright">
	<p>
    
      &copy; 2021
    .
    All rights reserved.
    
  </p>
</div>
</footer>



</section>
</article>
</div>
</body>
</html>
