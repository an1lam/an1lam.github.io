<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Learnings on Stephen Malina</title>
    <link>http://localhost:1314/learning/</link>
    <description>Recent content in Learnings on Stephen Malina</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 12 Dec 2019 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1314/learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>All of Statistics - Chapter 3</title>
      <link>http://localhost:1314/learning/2019-12-12-all-of-statistics-ch3-notes/</link>
      <pubDate>Thu, 12 Dec 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1314/learning/2019-12-12-all-of-statistics-ch3-notes/</guid>
      <description>&lt;h1 id=&#34;selected-exercises&#34;&gt;&#xA;Selected Exercises&#xA;&lt;a href=&#34;#selected-exercises&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;strong&gt;1.&lt;/strong&gt; Suppose we play a game where we start with $ c $ dollars. On each play of the game you either double or halve your money, with equal probability. What is your expected fortune after $ n $ trials?&lt;/p&gt;&#xA;&lt;p&gt;I&amp;rsquo;m pretty sure we can use a trick that I&amp;rsquo;ve seen a lot in machine learning to solve this using plain old expectations. If I&amp;rsquo;m right, we let $ X \sim \text{Binomial}(n, \frac{1}{2}) $ and can create a new random variable, $ Z $, where&lt;/p&gt;</description>
    </item>
    <item>
      <title>Causal Inference Notes</title>
      <link>http://localhost:1314/learning/2019-11-08-causal-inference/</link>
      <pubDate>Fri, 15 Nov 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1314/learning/2019-11-08-causal-inference/</guid>
      <description>&lt;h2 id=&#34;causal-inference-in-statistics&#34;&gt;&#xA;Causal Inference in Statistics&#xA;&lt;a href=&#34;#causal-inference-in-statistics&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h3 id=&#34;questions&#34;&gt;&#xA;Questions&#xA;&lt;a href=&#34;#questions&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Why is the causal effect identifiable in an IV DAG when the dependencies are linear (from an SCM perspective)?&#xA;&lt;ul&gt;&#xA;&lt;li&gt;I think this may relate to factor models.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;advanced-data-analysis-from-an-elementary-point-of-view-ch-18-23&#34;&gt;&#xA;Advanced Data Analysis from an Elementary Point of View (Ch. 18-23)&#xA;&lt;a href=&#34;#advanced-data-analysis-from-an-elementary-point-of-view-ch-18-23&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h3 id=&#34;chapter-18&#34;&gt;&#xA;Chapter 18&#xA;&lt;a href=&#34;#chapter-18&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;h4 id=&#34;exercises&#34;&gt;&#xA;Exercises&#xA;&lt;a href=&#34;#exercises&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h4&gt;&#xA;&lt;p&gt;&lt;strong&gt;18.2.&lt;/strong&gt;&#xA;&lt;em&gt;Proof that every path must go through a collider:&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;Observe that every path between two exogenous variables starts with variables going in the opposite direction. Thus, if we imagine walking the path from one exogenous variable to another, we see that the direction of the arrows has to switch at a node, which will be a collider.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Paper Review - IVs and Mendelian Randomization</title>
      <link>http://localhost:1314/learning/2019-11-02-ivs-mendelian-randomization/</link>
      <pubDate>Sun, 03 Nov 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1314/learning/2019-11-02-ivs-mendelian-randomization/</guid>
      <description>&lt;h1 id=&#34;summary&#34;&gt;&#xA;Summary&#xA;&lt;a href=&#34;#summary&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h1 id=&#34;detailed-notes&#34;&gt;&#xA;Detailed Notes&#xA;&lt;a href=&#34;#detailed-notes&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h2 id=&#34;3-iv-requirements&#34;&gt;&#xA;3 IV Requirements&#xA;&lt;a href=&#34;#3-iv-requirements&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;IV must have a direct influence on the treatment&lt;/li&gt;&#xA;&lt;li&gt;IV must not covary with the unmeasured confounding that impacts the outcome&lt;/li&gt;&#xA;&lt;li&gt;IV must not have a direct influence on the outcome&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;relevant-considerations-for-deciding-whether-to-do-an-iv-analysis&#34;&gt;&#xA;Relevant considerations for deciding whether to do an IV analysis&#xA;&lt;a href=&#34;#relevant-considerations-for-deciding-whether-to-do-an-iv-analysis&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Is there any unmeasured confounding?&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;If not, is the answer that we can just use a normal regression?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;When is unmeasured confounding especially likely?&lt;/p&gt;</description>
    </item>
    <item>
      <title>Linear Algebra Done Right - Chapter 6</title>
      <link>http://localhost:1314/learning/2019-05-20-linear-algebra-done-right-ch6-notes/</link>
      <pubDate>Mon, 20 May 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1314/learning/2019-05-20-linear-algebra-done-right-ch6-notes/</guid>
      <description>&lt;h1 id=&#34;selected-exercises&#34;&gt;&#xA;Selected Exercises&#xA;&lt;a href=&#34;#selected-exercises&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h2 id=&#34;6b&#34;&gt;&#xA;6.B&#xA;&lt;a href=&#34;#6b&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;1.&lt;/strong&gt; (a) Suppose \( \theta \in \mathbf{R} \). Show that $ (\cos \theta, \sin \theta), (-\sin \theta, \cos \theta) $ and $ (\cos \theta, \sin \theta), (\sin \theta, -\cos \theta) $ are orthonormal bases of $ \mathbf{R}^2 $.&lt;br&gt;&#xA;(b) Show that each orthonormal basis of $ \mathbf{R}^2 $ is of the form given by one of the two possibilities of part (a).&lt;/p&gt;&#xA;&lt;p&gt;(a) First, we show that both lists of vectors are orthonormal (using the Euclidean inner product), i.e. that the inner product of the two vectors equals 0 and that the inner product of each with itself equals 1.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Linear Algebra Done Right - Chapter 5</title>
      <link>http://localhost:1314/learning/2019-04-01-linear-algebra-done-right-ch5-notes/</link>
      <pubDate>Mon, 01 Apr 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1314/learning/2019-04-01-linear-algebra-done-right-ch5-notes/</guid>
      <description>&lt;h1 id=&#34;selected-exercises&#34;&gt;&#xA;Selected Exercises&#xA;&lt;a href=&#34;#selected-exercises&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h2 id=&#34;5a&#34;&gt;&#xA;5.A&#xA;&lt;a href=&#34;#5a&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;12.&lt;/strong&gt; Define $ T \in \mathcal L(\mathcal P_4(\mathbf{R})) $ by&lt;/p&gt;&#xA;&lt;p&gt;$$&#xA;(Tp)(x) = xp&amp;rsquo;(x)&#xA;$$&lt;/p&gt;&#xA;&lt;p&gt;for all $ x \in \mathbf{R} $. Find all eigenvalues and eigenvectors of $ T $.&lt;/p&gt;&#xA;&lt;p&gt;Observe that, if $ p = a_0 + a_1 x + a_2 x^2 + a_3 x^3 + a_4 x^4 $, then&#xA;$$&#xA;x p&amp;rsquo;(x) = a_1 x + 2 a_2 x^2 + 3 a_3 x^3 + 4 a_4 x^4.&#xA;$$&lt;/p&gt;</description>
    </item>
    <item>
      <title>Linear Algebra Done Right - Chapter 4</title>
      <link>http://localhost:1314/learning/2019-03-31-linear-algebra-done-right-ch4-notes/</link>
      <pubDate>Sun, 31 Mar 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1314/learning/2019-03-31-linear-algebra-done-right-ch4-notes/</guid>
      <description>&lt;h1 id=&#34;selected-exercises&#34;&gt;&#xA;Selected Exercises&#xA;&lt;a href=&#34;#selected-exercises&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;strong&gt;4.&lt;/strong&gt; Suppose $m$ and $n$ are positive integers with $ m \leq n $, and suppose $ \lambda_1, \dots, \lambda_m \in F $. Prove that there exists a polynomial $ p \in \mathcal P(\mathbf{F}) $ with $ \deg p = n $ such that $ 0 = p(\lambda_1) = \cdots = p(\lambda_m) $ and such that $ p $ has no other zeros.&lt;/p&gt;&#xA;&lt;p&gt;First, we can show that the polynomial $p&amp;rsquo;(z) = (z - \lambda_1) \cdots (z-\lambda_m) $ with $ \deg p&amp;rsquo; = m $ has $ 0 = p&amp;rsquo;(\lambda_1) = \cdots = p&amp;rsquo;(\lambda_m) $ and no other zeros. By 4.12, we know that $ p&amp;rsquo; $ has at most $ m $ zeros and therefore has no other zeros besides $ \lambda_1, \dots, \lambda_m $.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Linear Algebra Done Right - Chapter 3</title>
      <link>http://localhost:1314/learning/2019-03-30-linear-algebra-done-right-ch3-notes/</link>
      <pubDate>Sat, 30 Mar 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1314/learning/2019-03-30-linear-algebra-done-right-ch3-notes/</guid>
      <description>&lt;h1 id=&#34;selected-exercises&#34;&gt;&#xA;Selected Exercises&#xA;&lt;a href=&#34;#selected-exercises&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h2 id=&#34;3d&#34;&gt;&#xA;3.D&#xA;&lt;a href=&#34;#3d&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;7.&lt;/strong&gt; Suppose $ V $ &amp;amp; $ W $ are finite-dimensional. Let $ v \in V $. Let&#xA;$$&#xA;E = \{ T \in \mathcal L(V, W): T(v) = 0 \}.&#xA;$$&lt;/p&gt;&#xA;&lt;p&gt;(a)  Show that $ E $ is a subspace of $ \mathcal L(V, W) $.&lt;br&gt;&#xA;(b)  Suppose $ v \neq 0 $. What is $\dim E$?&lt;/p&gt;&#xA;&lt;p&gt;For (a), to show $ E $ is a subspace of $ \mathcal L(V, W) $, we need to show that $ E $ contains zero and is closed under both addition and multiplication.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Linear Algebra Done Right - Chapter 2</title>
      <link>http://localhost:1314/learning/2018-12-24-linear-algebra-done-right-ch2-notes/</link>
      <pubDate>Mon, 24 Dec 2018 00:00:00 +0000</pubDate>
      <guid>http://localhost:1314/learning/2018-12-24-linear-algebra-done-right-ch2-notes/</guid>
      <description>&lt;h1 id=&#34;intro&#34;&gt;&#xA;Intro&#xA;&lt;a href=&#34;#intro&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;I&amp;rsquo;ve recently been making my way through Axler&amp;rsquo;s &lt;em&gt;Linear Algebra Done Right&lt;/em&gt; and, as a way to motivate myself to continue, have decided to blog my notes and solutions for exercises as I go.&lt;/p&gt;&#xA;&lt;h1 id=&#34;insights&#34;&gt;&#xA;Insights&#xA;&lt;a href=&#34;#insights&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h2 id=&#34;section-2a&#34;&gt;&#xA;Section 2.A&#xA;&lt;a href=&#34;#section-2a&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h4 id=&#34;you-can-convert-any-linearly-dependent-list-to-a-linearly-independent-list-with-the-same-span&#34;&gt;&#xA;You can convert any linearly dependent list to a linearly independent list with the same span.&#xA;&lt;a href=&#34;#you-can-convert-any-linearly-dependent-list-to-a-linearly-independent-list-with-the-same-span&#34; class=&#34;heading-anchor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h4&gt;&#xA;&lt;p&gt;By the linear dependence lemma, if you have a list that&amp;rsquo;s linearly dependenty, then you can remove one item without changing the list&amp;rsquo;s span. By inductive hand-waving, that means we can remove one item repeatedly while not changing the list&amp;rsquo;s span until the list becomes linearly independent. This seems like it might be useful by analogy to compression, if we assume the span captures some essential property of a list of vectors, we can remove items from a linearly dependent list without changing its final representation.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
