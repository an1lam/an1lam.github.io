<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Learnings on Stephen Malina</title>
    <link>https://an1lam.github.io/learning/</link>
    <description>Recent content in Learnings on Stephen Malina</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 12 Dec 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://an1lam.github.io/learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>All of Statistics - Chapter 3</title>
      <link>https://an1lam.github.io/learning/2019-12-12-all-of-statistics-ch3-notes/</link>
      <pubDate>Thu, 12 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://an1lam.github.io/learning/2019-12-12-all-of-statistics-ch3-notes/</guid>
      <description>Selected Exercises 1. Suppose we play a game where we start with $ c $ dollars. On each play of the game you either double or halve your money, with equal probability. What is your expected fortune after $ n $ trials?
I&#39;m pretty sure we can use a trick that I&#39;ve seen a lot in machine learning to solve this using plain old expectations. If I&#39;m right, we let $ X \sim \text{Binomial}(n, \frac{1}{2}) $ and can create a new random variable, $ Z $, where</description>
    </item>
    
    <item>
      <title>Causal Inference Notes</title>
      <link>https://an1lam.github.io/learning/2019-11-08-causal-inference/</link>
      <pubDate>Fri, 15 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://an1lam.github.io/learning/2019-11-08-causal-inference/</guid>
      <description>Causal Inference in Statistics Questions  Why is the causal effect identifiable in an IV DAG when the dependencies are linear (from an SCM perspective)?  I think this may relate to factor models.    Advanced Data Analysis from an Elementary Point of View (Ch. 18-23) Chapter 18 Exercises 18.2. Proof that every path must go through a collider:
Observe that every path between two exogenous variables starts with variables going in the opposite direction.</description>
    </item>
    
    <item>
      <title>Paper Review - IVs and Mendelian Randomization</title>
      <link>https://an1lam.github.io/learning/2019-11-02-ivs-mendelian-randomization/</link>
      <pubDate>Sun, 03 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://an1lam.github.io/learning/2019-11-02-ivs-mendelian-randomization/</guid>
      <description>Summary Detailed Notes 3 IV Requirements  IV must have a direct influence on the treatment IV must not covary with the unmeasured confounding that impacts the outcome IV must not have a direct influence on the outcome  Relevant considerations for deciding whether to do an IV analysis   Is there any unmeasured confounding?
 If not, is the answer that we can just use a normal regression?    When is unmeasured confounding especially likely?</description>
    </item>
    
    <item>
      <title>Linear Algebra Done Right - Chapter 6</title>
      <link>https://an1lam.github.io/learning/2019-05-20-linear-algebra-done-right-ch6-notes/</link>
      <pubDate>Mon, 20 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://an1lam.github.io/learning/2019-05-20-linear-algebra-done-right-ch6-notes/</guid>
      <description>Selected Exercises 6.B 1. (a) Suppose \( \theta \in \mathbf{R} \). Show that $ (\cos \theta, \sin \theta), (-\sin \theta, \cos \theta) $ and $ (\cos \theta, \sin \theta), (\sin \theta, -\cos \theta) $ are orthonormal bases of $ \mathbf{R}^2 $.
(b) Show that each orthonormal basis of $ \mathbf{R}^2 $ is of the form given by one of the two possibilities of part (a).
(a) First, we show that both lists of vectors are orthonormal (using the Euclidean inner product), i.</description>
    </item>
    
    <item>
      <title>Linear Algebra Done Right - Chapter 5</title>
      <link>https://an1lam.github.io/learning/2019-04-01-linear-algebra-done-right-ch5-notes/</link>
      <pubDate>Mon, 01 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://an1lam.github.io/learning/2019-04-01-linear-algebra-done-right-ch5-notes/</guid>
      <description>Selected Exercises 5.A 12. Define $ T \in \mathcal L(\mathcal P_4(\mathbf{R})) $ by
$$ (Tp)(x) = xp&amp;rsquo;(x) $$
for all $ x \in \mathbf{R} $. Find all eigenvalues and eigenvectors of $ T $.
Observe that, if $ p = a_0 + a_1 x + a_2 x^2 + a_3 x^3 + a_4 x^4 $, then $$ x p&amp;rsquo;(x) = a_1 x + 2 a_2 x^2 + 3 a_3 x^3 + 4 a_4 x^4.</description>
    </item>
    
    <item>
      <title>Linear Algebra Done Right - Chapter 4</title>
      <link>https://an1lam.github.io/learning/2019-03-31-linear-algebra-done-right-ch4-notes/</link>
      <pubDate>Sun, 31 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://an1lam.github.io/learning/2019-03-31-linear-algebra-done-right-ch4-notes/</guid>
      <description>Selected Exercises 4. Suppose $m$ and $n$ are positive integers with $ m \leq n $, and suppose $ \lambda_1, \dots, \lambda_m \in F $. Prove that there exists a polynomial $ p \in \mathcal P(\mathbf{F}) $ with $ \deg p = n $ such that $ 0 = p(\lambda_1) = \cdots = p(\lambda_m) $ and such that $ p $ has no other zeros.
First, we can show that the polynomial $p&amp;rsquo;(z) = (z - \lambda_1) \cdots (z-\lambda_m) $ with $ \deg p&amp;rsquo; = m $ has $ 0 = p&amp;rsquo;(\lambda_1) = \cdots = p&amp;rsquo;(\lambda_m) $ and no other zeros.</description>
    </item>
    
    <item>
      <title>Linear Algebra Done Right - Chapter 3</title>
      <link>https://an1lam.github.io/learning/2019-03-30-linear-algebra-done-right-ch3-notes/</link>
      <pubDate>Sat, 30 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://an1lam.github.io/learning/2019-03-30-linear-algebra-done-right-ch3-notes/</guid>
      <description>Selected Exercises 3.D 7. Suppose $ V $ &amp;amp; $ W $ are finite-dimensional. Let $ v \in V $. Let $$ E = \{ T \in \mathcal L(V, W): T(v) = 0 \}. $$
(a) Show that $ E $ is a subspace of $ \mathcal L(V, W) $.
(b) Suppose $ v \neq 0 $. What is $\dim E$?
For (a), to show $ E $ is a subspace of $ \mathcal L(V, W) $, we need to show that $ E $ contains zero and is closed under both addition and multiplication.</description>
    </item>
    
    <item>
      <title>Linear Algebra Done Right - Chapter 2</title>
      <link>https://an1lam.github.io/learning/2018-12-24-linear-algebra-done-right-ch2-notes/</link>
      <pubDate>Mon, 24 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://an1lam.github.io/learning/2018-12-24-linear-algebra-done-right-ch2-notes/</guid>
      <description>Intro I&#39;ve recently been making my way through Axler&#39;s Linear Algebra Done Right and, as a way to motivate myself to continue, have decided to blog my notes and solutions for exercises as I go.
Insights Section 2.A You can convert any linearly dependent list to a linearly independent list with the same span. By the linear dependence lemma, if you have a list that&#39;s linearly dependenty, then you can remove one item without changing the list&#39;s span.</description>
    </item>
    
  </channel>
</rss>